{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zohair['congress']\t 1e-06\n",
      "ateeb['congress']\t 1e-06\n",
      "doc_count['congress']\t 0\n",
      "n_words 263\n",
      "------------------------------\n",
      "Zohair Term\t 0.0\n",
      "Ateeb Term\t 0.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import count\n",
    "import operator\n",
    "import math\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "\n",
    "def calc_term_doc_given_author(prob_map, counts):\n",
    "    \"\"\"\n",
    "    How likely is the document, given the counts of words in the doc\n",
    "    and the authors prob_map\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    for word in counts:\n",
    "        word_prob = get_word_prob(prob_map, word)\n",
    "        word_count = counts[word]\n",
    "        prob *= word_prob ** word_count  # Multiply probabilities based on word frequency\n",
    "    return prob\n",
    "\n",
    "\n",
    "# If a word is in a probability dictionary, return its probability\n",
    "# otherwise, return epsilon\n",
    "def get_word_prob(word_prob_map, word):\n",
    "    \"\"\"\n",
    "    Gets probability of a word:\n",
    "    Returns probability if word exists, EPSILON if not\n",
    "    \"\"\"\n",
    "    if word in word_prob_map:\n",
    "        return word_prob_map[word]\n",
    "    return EPSILON\n",
    "\n",
    "\n",
    "# From a file name, approximate the probability of a word\n",
    "# being generated from the same distribution as the file.\n",
    "# Assume that each word is produced independently, regardless\n",
    "# of order.\n",
    "def make_word_prob_map(fileName):\n",
    "    \"\"\"\n",
    "    Calculates word probabilities:\n",
    "    1. Counts word frequencies\n",
    "    2. Converts counts to probabilities\n",
    "    Returns: word probability dictionary\n",
    "    \"\"\"\n",
    "    wordMap, nWords = make_word_count_map(fileName)\n",
    "    probabilityMap = {}\n",
    "    for word in wordMap:\n",
    "        count = wordMap[word]\n",
    "        p = float(count) / nWords\n",
    "        probabilityMap[word] = p\n",
    "    return probabilityMap\n",
    "\n",
    "\n",
    "# From a file name, count the number of times each word exists\n",
    "# in that file. Return the result as a map (aka a dictionary)\n",
    "def make_word_count_map(fileName):\n",
    "    \"\"\"\n",
    "    Reads a file and counts word frequencies:\n",
    "    1. Opens file\n",
    "    2. Splits into words\n",
    "    3. Standardizes each word\n",
    "    4. Counts occurrences\n",
    "    Returns: (wordMap, total word count)\n",
    "    \"\"\"\n",
    "    wordMap = {}\n",
    "    nWords = 0\n",
    "    with open(fileName ,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                word = standardize(word)\n",
    "                add_word_to_count_map(wordMap, word)\n",
    "                nWords+= 1\n",
    "    return wordMap, nWords\n",
    "\n",
    "\n",
    "# Add a word to a count map. Makes sure not to crash if the\n",
    "# word has not been seen before.\n",
    "def add_word_to_count_map(wordMap, word):\n",
    "    \"\"\"\n",
    "    Updates word count in dictionary:\n",
    "    1. Skips stop words\n",
    "    2. Initializes count if new word\n",
    "    3. Increments count if existing word\n",
    "    \"\"\"\n",
    "    if is_stop(word):\n",
    "        return\n",
    "    if not word in wordMap:\n",
    "        wordMap[word] = 0\n",
    "    wordMap[word] += 1\n",
    "\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    Standardizes words by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation\n",
    "    3. Keeping only alphabetic characters\n",
    "    \"\"\"\n",
    "    standard = word.lower().strip()\n",
    "    # remove punctuation\n",
    "    standard = ''.join([i for i in standard if i.isalpha()])\n",
    "    return standard\n",
    "\n",
    "\n",
    "def is_stop(word):\n",
    "    \"\"\"\n",
    "    Removes common words that don't help in analysis\n",
    "    \"\"\"\n",
    "    stop_words = ['to', 'i', 'the', 'and', 'of']\n",
    "    return word in stop_words\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Calculate all the ps and qs\n",
    "    # Eg zohairWordProb['congress'] = 0.005\n",
    "    # zohair_word_prob['piech'] = 0.0\n",
    "    # zohair_word_prob['the'] = 0.001\n",
    "\n",
    "    zohair_word_prob = make_word_prob_map('zohair.txt')\n",
    "    ateeb_word_prob = make_word_prob_map('ateeb.txt')\n",
    "\n",
    "    # Get the word count of the unknown document\n",
    "    # Eg unknown_doc_count['congress'] = 5\n",
    "    unknown_doc_count, n_words = make_word_count_map('unknown.txt')\n",
    "\n",
    "    # Check if 'congress' exists in the probability maps before printing\n",
    "    print(\"zohair['congress']\\t\", zohair_word_prob.get('congress', EPSILON))  # Use get() to avoid KeyError\n",
    "    print(\"ateeb['congress']\\t\", ateeb_word_prob.get('congress', EPSILON))  # Use get() to avoid KeyError\n",
    "    print(\"doc_count['congress']\\t\", unknown_doc_count.get('congress', 0))  # Use get() to avoid KeyError\n",
    "    print(\"n_words\", n_words)\n",
    "\n",
    "    zohair_term = calc_term_doc_given_author(zohair_word_prob, unknown_doc_count)\n",
    "    print('---'*10)\n",
    "    ateeb_term = calc_term_doc_given_author(ateeb_word_prob, unknown_doc_count)\n",
    "    print(\"Zohair Term\\t\", zohair_term)\n",
    "    print(\"Ateeb Term\\t\", ateeb_term)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zohair['judiciary']\t 1e-06\n",
      "ateeb['judiciary']\t 0.01384083044982699\n",
      "doc_count['judiciary']\t 0\n",
      "n_words 263\n",
      "word:  crime | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.003115264797507788\n",
      "word:  report | prob_word:  0.003115264797507788 | c_i:  1 | prob:  9.70487475859124e-06\n",
      "word:  midnight | prob_word:  1e-06 | c_i:  3 | prob:  9.704874758591239e-24\n",
      "word:  arsons | prob_word:  1e-06 | c_i:  3 | prob:  9.704874758591237e-42\n",
      "word:   | prob_word:  0.01557632398753894 | c_i:  4 | prob:  5.712810560841155e-49\n",
      "word:  city | prob_word:  1e-06 | c_i:  2 | prob:  5.7128105608411546e-61\n",
      "word:  brookdale | prob_word:  1e-06 | c_i:  2 | prob:  5.712810560841154e-73\n",
      "word:  has | prob_word:  0.003115264797507788 | c_i:  2 | prob:  5.544211101252078e-78\n",
      "word:  been | prob_word:  0.003115264797507788 | c_i:  2 | prob:  5.380587437284264e-83\n",
      "word:  rocked | prob_word:  1e-06 | c_i:  1 | prob:  5.3805874372842635e-89\n",
      "word:  by | prob_word:  1e-06 | c_i:  2 | prob:  5.380587437284263e-101\n",
      "word:  a | prob_word:  0.03426791277258567 | c_i:  7 | prob:  2.985680024847729e-111\n",
      "word:  series | prob_word:  0.003115264797507788 | c_i:  1 | prob:  9.301183878030308e-114\n",
      "word:  deliberate | prob_word:  1e-06 | c_i:  1 | prob:  9.301183878030307e-120\n",
      "word:  fires | prob_word:  1e-06 | c_i:  3 | prob:  9.301183878030306e-138\n",
      "word:  set | prob_word:  1e-06 | c_i:  1 | prob:  9.301183878030306e-144\n",
      "word:  under | prob_word:  1e-06 | c_i:  1 | prob:  9.301183878030306e-150\n",
      "word:  cover | prob_word:  1e-06 | c_i:  1 | prob:  9.301183878030305e-156\n",
      "word:  darkness | prob_word:  1e-06 | c_i:  1 | prob:  9.301183878030304e-162\n",
      "word:  over | prob_word:  0.003115264797507788 | c_i:  1 | prob:  2.8975650710374778e-164\n",
      "word:  past | prob_word:  0.003115264797507788 | c_i:  1 | prob:  9.026682464291207e-167\n",
      "word:  four | prob_word:  1e-06 | c_i:  1 | prob:  9.026682464291206e-173\n",
      "word:  months | prob_word:  0.003115264797507788 | c_i:  1 | prob:  2.8120506119287247e-175\n",
      "word:  at | prob_word:  0.01557632398753894 | c_i:  3 | prob:  1.0627180297347403e-180\n",
      "word:  least | prob_word:  1e-06 | c_i:  1 | prob:  1.0627180297347402e-186\n",
      "word:  eight | prob_word:  1e-06 | c_i:  1 | prob:  1.0627180297347401e-192\n",
      "word:  buildingsranging | prob_word:  1e-06 | c_i:  1 | prob:  1.06271802973474e-198\n",
      "word:  from | prob_word:  0.003115264797507788 | c_i:  2 | prob:  1.0313545382272493e-203\n",
      "word:  abandoned | prob_word:  1e-06 | c_i:  1 | prob:  1.0313545382272492e-209\n",
      "word:  warehouses | prob_word:  1e-06 | c_i:  1 | prob:  1.0313545382272491e-215\n",
      "word:  occupied | prob_word:  1e-06 | c_i:  1 | prob:  1.0313545382272491e-221\n",
      "word:  residential | prob_word:  1e-06 | c_i:  1 | prob:  1.0313545382272491e-227\n",
      "word:  complexeshave | prob_word:  1e-06 | c_i:  1 | prob:  1.0313545382272491e-233\n",
      "word:  gone | prob_word:  1e-06 | c_i:  1 | prob:  1.031354538227249e-239\n",
      "word:  up | prob_word:  1e-06 | c_i:  1 | prob:  1.031354538227249e-245\n",
      "word:  in | prob_word:  0.006230529595015576 | c_i:  1 | prob:  6.4258849733784985e-248\n",
      "word:  flames | prob_word:  1e-06 | c_i:  1 | prob:  6.425884973378498e-254\n",
      "word:  investigators | prob_word:  1e-06 | c_i:  1 | prob:  6.4258849733784975e-260\n",
      "word:  have | prob_word:  0.01557632398753894 | c_i:  3 | prob:  2.4284427133859708e-265\n",
      "word:  identified | prob_word:  1e-06 | c_i:  1 | prob:  2.428442713385971e-271\n",
      "word:  pattern | prob_word:  0.003115264797507788 | c_i:  1 | prob:  7.56524209777561e-274\n",
      "word:  accelerants | prob_word:  1e-06 | c_i:  1 | prob:  7.565242097775609e-280\n",
      "word:  found | prob_word:  1e-06 | c_i:  1 | prob:  7.565242097775609e-286\n",
      "word:  scene | prob_word:  1e-06 | c_i:  1 | prob:  7.565242097775609e-292\n",
      "word:  fire | prob_word:  1e-06 | c_i:  3 | prob:  7.5652420977756e-310\n",
      "word:  alarms | prob_word:  1e-06 | c_i:  1 | prob:  7.5652421e-316\n",
      "word:  mysteriously | prob_word:  1e-06 | c_i:  1 | prob:  7.56e-322\n",
      "word:  disabled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  security | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  footage | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  corrupted | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  beyond | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  retrieval | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  these | prob_word:  0.006230529595015576 | c_i:  2 | prob:  0.0\n",
      "word:  calculated | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  actions | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suggest | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  skilled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  arsonist | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  with | prob_word:  0.006230529595015576 | c_i:  2 | prob:  0.0\n",
      "word:  deep | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  understanding | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  both | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  technology | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  one | prob_word:  0.009345794392523364 | c_i:  1 | prob:  0.0\n",
      "word:  most | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  devastating | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  incidents | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  occurred | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  historic | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  greystone | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  apartments | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  centuryold | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  complex | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  that | prob_word:  0.009345794392523364 | c_i:  1 | prob:  0.0\n",
      "word:  housed | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  nearly | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  fifty | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  residents | prob_word:  0.003115264797507788 | c_i:  2 | prob:  0.0\n",
      "word:  broke | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  out | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  just | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  after | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  spreading | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  rapidly | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  due | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  aged | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  wooden | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  structure | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  miraculously | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  no | prob_word:  0.018691588785046728 | c_i:  2 | prob:  0.0\n",
      "word:  lives | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  were | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  lost | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  but | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  several | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  tenants | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suffered | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  severe | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  smoke | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  inhalation | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  firefighters | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  struggled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  contain | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  inferno | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  battling | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  intense | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  heat | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  unexpected | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  structural | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  collapses | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  authorities | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  speculated | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  on | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  motivations | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  behind | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  some | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  believe | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  are | prob_word:  0.009345794392523364 | c_i:  1 | prob:  0.0\n",
      "word:  acts | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  revenge | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  while | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  others | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  point | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  financial | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  incentives | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  considering | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  few | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  affected | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  properties | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  recently | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  denied | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  demolition | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  permits | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  however | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  clear | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suspect | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  emerged | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  forensic | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  experts | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  remain | prob_word:  0.006230529595015576 | c_i:  2 | prob:  0.0\n",
      "word:  puzzled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  meticulous | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  nature | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  crimes | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  each | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  new | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  blaze | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  fear | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  tightens | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  its | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  grip | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  as | prob_word:  0.009345794392523364 | c_i:  2 | prob:  0.0\n",
      "word:  dread | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  where | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  will | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  strike | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  next | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  despite | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  increased | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  patrols | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  public | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  awareness | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  campaigns | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  continue | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  they | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  dubbed | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  unsolved | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  leaving | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  trail | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  destruction | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  uncertainty | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  waits | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  anxiously | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  for | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  law | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  enforcement | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  piece | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  together | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  puzzle | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  before | prob_word:  0.003115264797507788 | c_i:  1 | prob:  0.0\n",
      "word:  another | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  landmark | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  is | prob_word:  0.006230529595015576 | c_i:  1 | prob:  0.0\n",
      "word:  reduced | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  ashes | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "------------------------------\n",
      "word:  crime | prob_word:  1e-06 | c_i:  1 | prob:  1e-06\n",
      "word:  report | prob_word:  1e-06 | c_i:  1 | prob:  1e-12\n",
      "word:  midnight | prob_word:  1e-06 | c_i:  3 | prob:  9.999999999999999e-31\n",
      "word:  arsons | prob_word:  1e-06 | c_i:  3 | prob:  9.999999999999998e-49\n",
      "word:   | prob_word:  0.01730103806228374 | c_i:  4 | prob:  8.959600520605316e-56\n",
      "word:  city | prob_word:  1e-06 | c_i:  2 | prob:  8.959600520605316e-68\n",
      "word:  brookdale | prob_word:  1e-06 | c_i:  2 | prob:  8.959600520605315e-80\n",
      "word:  has | prob_word:  0.006920415224913495 | c_i:  2 | prob:  4.29094504165674e-84\n",
      "word:  been | prob_word:  1e-06 | c_i:  2 | prob:  4.29094504165674e-96\n",
      "word:  rocked | prob_word:  1e-06 | c_i:  1 | prob:  4.29094504165674e-102\n",
      "word:  by | prob_word:  1e-06 | c_i:  2 | prob:  4.29094504165674e-114\n",
      "word:  a | prob_word:  1e-06 | c_i:  7 | prob:  4.290945041656739e-156\n",
      "word:  series | prob_word:  1e-06 | c_i:  1 | prob:  4.2909450416567385e-162\n",
      "word:  deliberate | prob_word:  1e-06 | c_i:  1 | prob:  4.290945041656738e-168\n",
      "word:  fires | prob_word:  1e-06 | c_i:  3 | prob:  4.290945041656738e-186\n",
      "word:  set | prob_word:  1e-06 | c_i:  1 | prob:  4.290945041656737e-192\n",
      "word:  under | prob_word:  1e-06 | c_i:  1 | prob:  4.290945041656737e-198\n",
      "word:  cover | prob_word:  1e-06 | c_i:  1 | prob:  4.2909450416567366e-204\n",
      "word:  darkness | prob_word:  1e-06 | c_i:  1 | prob:  4.290945041656736e-210\n",
      "word:  over | prob_word:  0.006920415224913495 | c_i:  1 | prob:  2.969512139554835e-212\n",
      "word:  past | prob_word:  1e-06 | c_i:  1 | prob:  2.969512139554835e-218\n",
      "word:  four | prob_word:  1e-06 | c_i:  1 | prob:  2.969512139554835e-224\n",
      "word:  months | prob_word:  1e-06 | c_i:  1 | prob:  2.9695121395548348e-230\n",
      "word:  at | prob_word:  0.0034602076124567475 | c_i:  3 | prob:  1.2302449097317277e-237\n",
      "word:  least | prob_word:  1e-06 | c_i:  1 | prob:  1.2302449097317278e-243\n",
      "word:  eight | prob_word:  1e-06 | c_i:  1 | prob:  1.2302449097317278e-249\n",
      "word:  buildingsranging | prob_word:  1e-06 | c_i:  1 | prob:  1.2302449097317277e-255\n",
      "word:  from | prob_word:  0.0034602076124567475 | c_i:  2 | prob:  1.4729767480414837e-260\n",
      "word:  abandoned | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414836e-266\n",
      "word:  warehouses | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414837e-272\n",
      "word:  occupied | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414835e-278\n",
      "word:  residential | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414835e-284\n",
      "word:  complexeshave | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414834e-290\n",
      "word:  gone | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414832e-296\n",
      "word:  up | prob_word:  1e-06 | c_i:  1 | prob:  1.4729767480414832e-302\n",
      "word:  in | prob_word:  0.01384083044982699 | c_i:  1 | prob:  2.03872214261797e-304\n",
      "word:  flames | prob_word:  1e-06 | c_i:  1 | prob:  2.038722142618e-310\n",
      "word:  investigators | prob_word:  1e-06 | c_i:  1 | prob:  2.03872216e-316\n",
      "word:  have | prob_word:  0.0034602076124567475 | c_i:  3 | prob:  1e-323\n",
      "word:  identified | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  pattern | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  accelerants | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  found | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  scene | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  fire | prob_word:  1e-06 | c_i:  3 | prob:  0.0\n",
      "word:  alarms | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  mysteriously | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  disabled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  security | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  footage | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  corrupted | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  beyond | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  retrieval | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  these | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  calculated | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  actions | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suggest | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  skilled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  arsonist | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  with | prob_word:  0.0034602076124567475 | c_i:  2 | prob:  0.0\n",
      "word:  deep | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  understanding | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  both | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  technology | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  one | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  most | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  devastating | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  incidents | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  occurred | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  historic | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  greystone | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  apartments | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  centuryold | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  complex | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  that | prob_word:  0.020761245674740483 | c_i:  1 | prob:  0.0\n",
      "word:  housed | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  nearly | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  fifty | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  residents | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  broke | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  out | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  just | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  after | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  spreading | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  rapidly | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  due | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  aged | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  wooden | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  structure | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  miraculously | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  no | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  lives | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  were | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  lost | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  but | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  several | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  tenants | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suffered | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  severe | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  smoke | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  inhalation | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  firefighters | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  struggled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  contain | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  inferno | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  battling | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  intense | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  heat | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  unexpected | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  structural | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  collapses | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  authorities | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  speculated | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  on | prob_word:  0.010380622837370242 | c_i:  2 | prob:  0.0\n",
      "word:  motivations | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  behind | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  some | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  believe | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  are | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  acts | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  revenge | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  while | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  others | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  point | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  financial | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  incentives | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  considering | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  few | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  affected | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  properties | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  recently | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  denied | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  demolition | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  permits | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  however | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  clear | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  suspect | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  emerged | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  forensic | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  experts | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  remain | prob_word:  1e-06 | c_i:  2 | prob:  0.0\n",
      "word:  puzzled | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  meticulous | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  nature | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  crimes | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  each | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  new | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  blaze | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  fear | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  tightens | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  its | prob_word:  0.006920415224913495 | c_i:  1 | prob:  0.0\n",
      "word:  grip | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  as | prob_word:  0.010380622837370242 | c_i:  2 | prob:  0.0\n",
      "word:  dread | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  where | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  will | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  strike | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  next | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  despite | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  increased | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  patrols | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  public | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  awareness | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  campaigns | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  continue | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  they | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  dubbed | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  unsolved | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  leaving | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  trail | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  destruction | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  uncertainty | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  waits | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  anxiously | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  for | prob_word:  0.006920415224913495 | c_i:  1 | prob:  0.0\n",
      "word:  law | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  enforcement | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  piece | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  together | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  puzzle | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  before | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  another | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  landmark | prob_word:  0.0034602076124567475 | c_i:  1 | prob:  0.0\n",
      "word:  is | prob_word:  0.006920415224913495 | c_i:  1 | prob:  0.0\n",
      "word:  reduced | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "word:  ashes | prob_word:  1e-06 | c_i:  1 | prob:  0.0\n",
      "Zohair Term\t 0.0\n",
      "Ateeb Term\t 0.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import count\n",
    "import operator\n",
    "import math\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "\n",
    "def calc_term_doc_given_author(prob_map, counts):\n",
    "    \"\"\"\n",
    "    How likely is the document, given the counts of words in the doc\n",
    "    and the authors prob_map\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    for word, c_i in counts.items():\n",
    "        p_word = get_word_prob(prob_map, word)\n",
    "        prob *= p_word ** c_i\n",
    "        print(\"word: \", word, \"| prob_word: \", p_word, \"| c_i: \", c_i, \"| prob: \", prob)\n",
    "    return prob\n",
    "\n",
    "\n",
    "# If a word is in a probability dictionary, return its probability\n",
    "# otherwise, return epsilon\n",
    "def get_word_prob(word_prob_map, word):\n",
    "    if word in word_prob_map:\n",
    "        return word_prob_map[word]\n",
    "    return EPSILON\n",
    "\n",
    "\n",
    "# From a file name, approximate the probability of a word\n",
    "# being generated from the same distribution as the file.\n",
    "# Assume that each word is produced independently, regardless\n",
    "# of order.\n",
    "def make_word_prob_map(fileName):\n",
    "    wordMap, nWords = make_word_count_map(fileName)\n",
    "    probabilityMap = {}\n",
    "    for word in wordMap:\n",
    "        count = wordMap[word]\n",
    "        p = float(count) / nWords\n",
    "        probabilityMap[word] = p\n",
    "    return probabilityMap\n",
    "\n",
    "\n",
    "# From a file name, count the number of times each word exists\n",
    "# in that file. Return the result as a map (aka a dictionary)\n",
    "def make_word_count_map(fileName):\n",
    "    wordMap = {}\n",
    "    nWords = 0\n",
    "    with open(fileName, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                word = standardize(word)\n",
    "                add_word_to_count_map(wordMap, word)\n",
    "                nWords += 1\n",
    "    return wordMap, nWords\n",
    "\n",
    "\n",
    "# Add a word to a count map. Makes sure not to crash if the\n",
    "# word has not been seen before.\n",
    "def add_word_to_count_map(wordMap, word):\n",
    "    if is_stop(word):\n",
    "        return\n",
    "    if word not in wordMap:\n",
    "        wordMap[word] = 0\n",
    "    wordMap[word] += 1\n",
    "\n",
    "\n",
    "# Standardizes a word. For now, we are just going to make it\n",
    "# lower case.\n",
    "def standardize(word):\n",
    "    standard = word.lower().strip()\n",
    "    # remove punctuation\n",
    "    standard = ''.join([i for i in standard if i.isalpha()])\n",
    "    return standard\n",
    "\n",
    "\n",
    "def is_stop(word):\n",
    "    stop_words = ['to', 'i', 'the', 'and', 'of']\n",
    "    return word in stop_words\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Calculate all the ps and qs\n",
    "    # Eg zohairWordProb['judiciary'] = 0.005\n",
    "    # zohair_word_prob['arson'] = 0.0\n",
    "    # zohair_word_prob['silent'] = 0.001\n",
    "\n",
    "    zohair_word_prob = make_word_prob_map('zohair.txt')  # Updated file name\n",
    "    ateeb_word_prob = make_word_prob_map('ateeb.txt')  # Updated file name\n",
    "\n",
    "    # Get the word count of the unknown document\n",
    "    # Eg unknown_doc_count['judiciary'] = 5\n",
    "    unknown_doc_count, n_words = make_word_count_map('unknown.txt')  # Updated file name\n",
    "\n",
    "    # Change 'judiciary' or 'arson' based on the words found in the text files\n",
    "    test_word = 'judiciary'  # You can change this word based on your requirement\n",
    "    \n",
    "    print(f\"zohair['{test_word}']\\t\", zohair_word_prob.get(test_word, EPSILON))  # Use get() to avoid KeyError\n",
    "    print(f\"ateeb['{test_word}']\\t\", ateeb_word_prob.get(test_word, EPSILON))  # Use get() to avoid KeyError\n",
    "    print(f\"doc_count['{test_word}']\\t\", unknown_doc_count.get(test_word, 0))  # Use get() to avoid KeyError\n",
    "    print(\"n_words\", n_words)\n",
    "\n",
    "    zohair_term = calc_term_doc_given_author(zohair_word_prob, unknown_doc_count)\n",
    "    print('---' * 10)\n",
    "    ateeb_term = calc_term_doc_given_author(ateeb_word_prob, unknown_doc_count)\n",
    "    print(f\"Zohair Term\\t\", zohair_term)\n",
    "    print(f\"Ateeb Term\\t\", ateeb_term)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zohair['judiciary']\t 1e-06\n",
      "ateeb['judiciary']\t 0.01384083044982699\n",
      "doc_count['judiciary']\t 0\n",
      "n_words 263\n",
      "------------------------------\n",
      "Log P(D|Zohair)\t -2611.2663936765807\n",
      "Log P(D|Ateeb)\t -2876.146408718518\n",
      "diff\t 264.88001504193744\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import count\n",
    "import operator\n",
    "import math\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "def main():\n",
    "    # Calculate all the ps and qs for each author\n",
    "    # Example: zohairWordProb['judiciary'] = 0.005\n",
    "    # ateeb_word_prob['silent'] = 0.0\n",
    "    # zohair_word_prob['arson'] = 0.001\n",
    "\n",
    "    zohair_word_prob = make_word_prob_map('zohair.txt')  # Changed to 'zohair.txt'\n",
    "    ateeb_word_prob = make_word_prob_map('ateeb.txt')    # Changed to 'ateeb.txt'\n",
    "\n",
    "    # Get the word count of the unknown document\n",
    "    # Example: unknown_doc_count['judiciary'] = 5\n",
    "    unknown_doc_count, n_words = make_word_count_map('unknown.txt')  # Changed to 'unknown.txt'\n",
    "\n",
    "    # Select a word to test against, for instance 'judiciary' or 'arson'\n",
    "    test_word = 'judiciary'  # Change this to any word relevant to your text files\n",
    "\n",
    "    # Print out word probabilities and document counts\n",
    "    print(f\"zohair['{test_word}']\\t\", zohair_word_prob.get(test_word, EPSILON))  # Avoiding KeyError\n",
    "    print(f\"ateeb['{test_word}']\\t\", ateeb_word_prob.get(test_word, EPSILON))  # Avoiding KeyError\n",
    "    print(f\"doc_count['{test_word}']\\t\", unknown_doc_count.get(test_word, 0))  # Avoiding KeyError\n",
    "    print(\"n_words\", n_words)\n",
    "\n",
    "    # Calculate log probabilities for both authors\n",
    "    zohair_term = calc_log_pr_doc_given_author(zohair_word_prob, unknown_doc_count)\n",
    "    print('---' * 10)\n",
    "    ateeb_term = calc_log_pr_doc_given_author(ateeb_word_prob, unknown_doc_count)\n",
    "\n",
    "    # Print out the calculated log probabilities\n",
    "    print(f\"Log P(D|Zohair)\\t\", zohair_term)\n",
    "    print(f\"Log P(D|Ateeb)\\t\", ateeb_term)\n",
    "\n",
    "    # Calculate and print the difference\n",
    "    print(f\"diff\\t\", zohair_term - ateeb_term)\n",
    "\n",
    "def calc_log_pr_doc_given_author(prob_map, counts):\n",
    "    \"\"\"\n",
    "    Calculate the log probability of the document, given the counts of words in the doc\n",
    "    and the author's probability map.\n",
    "    \"\"\"\n",
    "    log_prob = math.log(1)  # Start with the log of 1, since we are multiplying probabilities\n",
    "    for word_i, c_i in counts.items():\n",
    "        p_i = get_word_prob(prob_map, word_i)\n",
    "        log_prob += c_i * math.log(p_i)\n",
    "    return log_prob\n",
    "\n",
    "# If a word is in a probability dictionary, return its probability,\n",
    "# otherwise, return epsilon (a small probability to avoid zero probability)\n",
    "def get_word_prob(word_prob_map, word):\n",
    "    if word in word_prob_map:\n",
    "        return word_prob_map[word]\n",
    "    return EPSILON\n",
    "\n",
    "# Approximate the probability of a word being generated from the same distribution as the file.\n",
    "# Assume that each word is produced independently, regardless of order.\n",
    "def make_word_prob_map(fileName):\n",
    "    wordMap, nWords = make_word_count_map(fileName)\n",
    "    probabilityMap = {}\n",
    "    for word in wordMap:\n",
    "        count = wordMap[word]\n",
    "        p = float(count) / nWords\n",
    "        probabilityMap[word] = p\n",
    "    return probabilityMap\n",
    "\n",
    "# From a file name, count the number of times each word exists in that file.\n",
    "# Return the result as a map (i.e., a dictionary).\n",
    "def make_word_count_map(fileName):\n",
    "    wordMap = {}\n",
    "    nWords = 0\n",
    "    with open(fileName, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                word = standardize(word)\n",
    "                add_word_to_count_map(wordMap, word)\n",
    "                nWords += 1\n",
    "    return wordMap, nWords\n",
    "\n",
    "# Add a word to a count map. Ensures not to crash if the word hasn't been seen before.\n",
    "def add_word_to_count_map(wordMap, word):\n",
    "    if not word in wordMap:\n",
    "        wordMap[word] = 0\n",
    "    wordMap[word] += 1\n",
    "\n",
    "# Standardizes a word. Converts it to lowercase and removes any punctuation.\n",
    "def standardize(word):\n",
    "    standard = word.lower().strip()\n",
    "    # Remove punctuation\n",
    "    standard = ''.join([i for i in standard if i.isalpha()])\n",
    "    return standard\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
