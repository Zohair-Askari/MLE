{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "{'crime': 1, 'report': 1, 'midnight': 3, 'arsons': 3, '': 4, 'city': 2, 'brookdale': 2, 'has': 2, 'been': 2, 'rocked': 1, 'by': 2, 'a': 7, 'series': 1, 'deliberate': 1, 'fires': 3, 'set': 1, 'under': 1, 'cover': 1, 'darkness': 1, 'over': 1, 'past': 1, 'four': 1, 'months': 1, 'at': 3, 'least': 1, 'eight': 1, 'buildingsranging': 1, 'from': 2, 'abandoned': 1, 'warehouses': 1, 'occupied': 1, 'residential': 1, 'complexeshave': 1, 'gone': 1, 'up': 1, 'in': 1, 'flames': 1, 'investigators': 1, 'have': 3, 'identified': 1, 'pattern': 1, 'accelerants': 1, 'found': 1, 'scene': 1, 'fire': 3, 'alarms': 1, 'mysteriously': 1, 'disabled': 1, 'security': 1, 'footage': 1, 'corrupted': 1, 'beyond': 1, 'retrieval': 1, 'these': 2, 'calculated': 1, 'actions': 1, 'suggest': 1, 'skilled': 1, 'arsonist': 2, 'with': 2, 'deep': 1, 'understanding': 1, 'both': 1, 'technology': 1, 'one': 1, 'most': 1, 'devastating': 1, 'incidents': 1, 'occurred': 1, 'historic': 1, 'greystone': 1, 'apartments': 1, 'centuryold': 1, 'complex': 1, 'that': 1, 'housed': 1, 'nearly': 1, 'fifty': 1, 'residents': 2, 'broke': 1, 'out': 1, 'just': 1, 'after': 1, 'spreading': 1, 'rapidly': 1, 'due': 1, 'aged': 1, 'wooden': 1, 'structure': 1, 'miraculously': 1, 'no': 2, 'lives': 1, 'were': 2, 'lost': 1, 'but': 1, 'several': 1, 'tenants': 1, 'suffered': 1, 'severe': 1, 'smoke': 1, 'inhalation': 1, 'firefighters': 1, 'struggled': 1, 'contain': 1, 'inferno': 1, 'battling': 1, 'intense': 1, 'heat': 1, 'unexpected': 1, 'structural': 1, 'collapses': 1, 'authorities': 1, 'speculated': 1, 'on': 2, 'motivations': 1, 'behind': 1, 'some': 1, 'believe': 1, 'are': 1, 'acts': 1, 'revenge': 1, 'while': 1, 'others': 1, 'point': 1, 'financial': 1, 'incentives': 1, 'considering': 1, 'few': 1, 'affected': 1, 'properties': 1, 'recently': 1, 'denied': 1, 'demolition': 1, 'permits': 1, 'however': 1, 'clear': 1, 'suspect': 1, 'emerged': 1, 'forensic': 1, 'experts': 1, 'remain': 2, 'puzzled': 1, 'meticulous': 1, 'nature': 1, 'crimes': 1, 'each': 1, 'new': 1, 'blaze': 1, 'fear': 1, 'tightens': 1, 'its': 1, 'grip': 1, 'as': 2, 'dread': 1, 'where': 1, 'will': 1, 'strike': 1, 'next': 1, 'despite': 1, 'increased': 1, 'patrols': 1, 'public': 1, 'awareness': 1, 'campaigns': 1, 'continue': 1, 'they': 1, 'dubbed': 1, 'unsolved': 1, 'leaving': 1, 'trail': 1, 'destruction': 1, 'uncertainty': 1, 'waits': 1, 'anxiously': 1, 'for': 1, 'law': 1, 'enforcement': 1, 'piece': 1, 'together': 1, 'puzzle': 1, 'before': 1, 'another': 1, 'landmark': 1, 'is': 1, 'reduced': 1, 'ashes': 1}\n",
      "**************************************************\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import count\n",
    "import operator\n",
    "import math\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "\n",
    "def calc_term_doc_given_author(prob_map, counts):\n",
    "    \"\"\"\n",
    "    How likely is the document, given the counts of words in the doc\n",
    "    and the authors prob_map\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    return prob\n",
    "\n",
    "\n",
    "# If a word is in a probability dictionary, return its probability\n",
    "# otherwise, return epsilon\n",
    "def get_word_prob(word_prob_map, word):\n",
    "    \"\"\"\n",
    "    Gets probability of a word:\n",
    "    Returns probability if word exists, EPSILON if not\n",
    "    \"\"\"\n",
    "    if word in word_prob_map:\n",
    "        return word_prob_map[word]\n",
    "    return EPSILON\n",
    "\n",
    "# From a file name, approximate the probability of a word\n",
    "# being generated from the same distribution as the file.\n",
    "# Assume that each word is produced independently, regardless\n",
    "# of order.\n",
    "def make_word_prob_map(fileName):\n",
    "    \"\"\"\n",
    "    Calculates word probabilities:\n",
    "    1. Counts word frequencies\n",
    "    2. Converts counts to probabilities\n",
    "    Returns: word probability dictionary\n",
    "    \"\"\"\n",
    "    wordMap, nWords = make_word_count_map(fileName)\n",
    "    # print(fileName)\n",
    "    # print(\"----------------------------\")\n",
    "    # print(wordMap)\n",
    "    # print(\"----------------------------\")\n",
    "    # print(nWords)\n",
    "    # print(\"----------------------------\")\n",
    "    probabilityMap = {}\n",
    "    for word in wordMap:\n",
    "        count = wordMap[word]\n",
    "        p = float(count) / nWords\n",
    "        probabilityMap[word] = p\n",
    "    return probabilityMap\n",
    "\n",
    "# From a file name, count the number of times each word exists\n",
    "# in that file. Return the result as a map (aka a dictionary)\n",
    "def make_word_count_map(fileName):\n",
    "    \"\"\"\n",
    "    Reads a file and counts word frequencies:\n",
    "    1. Opens file\n",
    "    2. Splits into words\n",
    "    3. Standardizes each word\n",
    "    4. Counts occurrences\n",
    "    Returns: (wordMap, total word count)\n",
    "    \"\"\"\n",
    "    wordMap = {}\n",
    "    nWords = 0\n",
    "    with open(fileName ,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            words = line.split(' ')\n",
    "            for word in words:\n",
    "                word = standardize(word)\n",
    "                add_word_to_count_map(wordMap, word)\n",
    "                nWords+= 1\n",
    "    return wordMap, nWords\n",
    "\n",
    "# Add a word to a count map. Makes sure not to crash if the\n",
    "# word has not been seen before.\n",
    "def add_word_to_count_map(wordMap, word):\n",
    "    \"\"\"\n",
    "    Updates word count in dictionary:\n",
    "    1. Skips stop words\n",
    "    2. Initializes count if new word\n",
    "    3. Increments count if existing word\n",
    "    \"\"\"\n",
    "    if is_stop(word):\n",
    "        return\n",
    "    if not word in wordMap:\n",
    "        wordMap[word] = 0\n",
    "    wordMap[word] += 1\n",
    "\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    Standardizes words by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation\n",
    "    3. Keeping only alphabetic characters\n",
    "    \"\"\"\n",
    "    standard = word.lower().strip()\n",
    "    # remove punctuation\n",
    "    standard = ''.join([i for i in standard if i.isalpha()])\n",
    "    return standard\n",
    "\n",
    "def is_stop(word):\n",
    "    \"\"\"\n",
    "    Removes common words that don't help in analysis\n",
    "    \"\"\"\n",
    "    stop_words = ['to', 'i', 'the', 'and', 'of']\n",
    "    return word in stop_words\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Calculate all the ps and qs\n",
    "    # Eg zohairWordProb['congress'] = 0.005\n",
    "    # zohair_word_prob['piech'] = 0.0\n",
    "    # zohair_word_prob['the'] = 0.001\n",
    "\n",
    "    hamilton_word_prob = make_word_prob_map('zohair.txt')\n",
    "    madison_word_prob = make_word_prob_map('ateeb.txt')\n",
    "\n",
    "    # print(\"********************************************\")\n",
    "    # print(\"zohair_word_prob\", zohair_word_prob)\n",
    "    # print(\"----------------------------------------------\")\n",
    "    # print(\"ateeb_word_prob\", ateeb_word_prob)\n",
    "    # print(\"--------------------------------------\")\n",
    "    # print(\"********************************************\")\n",
    "\n",
    "    # print(zohair_word_prob[\"independent\"])\n",
    "    # print(ateeb_word_prob[\"independent\"])\n",
    "\n",
    "    unknown_doc_count, n_words = make_word_count_map('unknown.txt')\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(unknown_doc_count)\n",
    "    print(\"*\" * 50)\n",
    "    print(n_words)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zohair['midnight']\t Not Found\n",
      "ateeb['midnight']\t Not Found\n",
      "doc_count['midnight']\t 3\n",
      "Total words in unknown document: 259\n",
      "===================================\n",
      "Likelihood Scores:\n",
      "Zohair's probability: 0.0\n",
      "Ateeb's probability: 0.0\n",
      "The document's author is inconclusive.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import operator\n",
    "import math\n",
    "\n",
    "EPSILON = 0.000001  # Small probability for unseen words\n",
    "\n",
    "\n",
    "def calc_term_doc_given_author(prob_map, counts):\n",
    "    \"\"\"\n",
    "    Computes the probability of the document being written by a specific author.\n",
    "    Uses the independent word assumption (Bag of Words model).\n",
    "    \"\"\"\n",
    "    prob = 1.0\n",
    "    for word, count in counts.items():\n",
    "        p_word = get_word_prob(prob_map, word)\n",
    "        prob *= p_word ** count  # Probability raised to count frequency\n",
    "    return prob\n",
    "\n",
    "\n",
    "def get_word_prob(word_prob_map, word):\n",
    "    \"\"\"\n",
    "    Returns the probability of a word occurring in an author's vocabulary.\n",
    "    If the word is not found, return a small probability (EPSILON).\n",
    "    \"\"\"\n",
    "    return word_prob_map.get(word, EPSILON)\n",
    "\n",
    "\n",
    "def make_word_prob_map(file_name):\n",
    "    \"\"\"\n",
    "    Reads a file and calculates word probabilities.\n",
    "    Returns a dictionary where keys are words and values are probabilities.\n",
    "    \"\"\"\n",
    "    word_map, total_words = make_word_count_map(file_name)\n",
    "    probability_map = {word: count / total_words for word, count in word_map.items()}\n",
    "    return probability_map\n",
    "\n",
    "\n",
    "def make_word_count_map(file_name):\n",
    "    \"\"\"\n",
    "    Reads a file and counts word frequencies.\n",
    "    Returns: \n",
    "        - wordMap (dictionary with word counts)\n",
    "        - total word count\n",
    "    \"\"\"\n",
    "    word_map = {}\n",
    "    total_words = 0\n",
    "\n",
    "    try:\n",
    "        with open(file_name, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    word = standardize(word)\n",
    "                    if word:  # Ensure non-empty word\n",
    "                        add_word_to_count_map(word_map, word)\n",
    "                        total_words += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_name}' not found.\")\n",
    "        return {}, 0  # Return empty dictionary and zero words\n",
    "\n",
    "    return word_map, total_words\n",
    "\n",
    "\n",
    "def add_word_to_count_map(word_map, word):\n",
    "    \"\"\"\n",
    "    Updates the word count in the given dictionary.\n",
    "    \"\"\"\n",
    "    if is_stop(word):\n",
    "        return\n",
    "    word_map[word] = word_map.get(word, 0) + 1\n",
    "\n",
    "\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes words:\n",
    "        - Converts to lowercase\n",
    "        - Removes punctuation\n",
    "        - Keeps only alphabetic characters\n",
    "    \"\"\"\n",
    "    return ''.join(char for char in word.lower().strip() if char.isalpha())\n",
    "\n",
    "\n",
    "def is_stop(word):\n",
    "    \"\"\"\n",
    "    Filters out common stop words to improve classification accuracy.\n",
    "    \"\"\"\n",
    "    stop_words = {'to', 'i', 'the', 'and', 'of'}\n",
    "    return word in stop_words\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load probability maps for each author\n",
    "    zohair_word_prob = make_word_prob_map('zohair.txt')\n",
    "    ateeb_word_prob = make_word_prob_map('ateeb.txt')\n",
    "\n",
    "    # Load unknown document for classification\n",
    "    unknown_doc_count, n_words = make_word_count_map('unknown.txt')\n",
    "\n",
    "    if n_words == 0:\n",
    "        print(\"Error: Unknown document is empty or missing.\")\n",
    "        return\n",
    "\n",
    "    # Safely access word probability without KeyError\n",
    "    print(\"zohair['midnight']\\t\", zohair_word_prob.get('midnight', 'Not Found'))\n",
    "    print(\"ateeb['midnight']\\t\", ateeb_word_prob.get('midnight', 'Not Found'))\n",
    "    print(\"doc_count['midnight']\\t\", unknown_doc_count.get('midnight', 'Not Found'))\n",
    "\n",
    "    print(\"Total words in unknown document:\", n_words)\n",
    "\n",
    "    # Compute likelihood of each author having written the document\n",
    "    zohair_term = calc_term_doc_given_author(zohair_word_prob, unknown_doc_count)\n",
    "    ateeb_term = calc_term_doc_given_author(ateeb_word_prob, unknown_doc_count)\n",
    "\n",
    "    print(\"===================================\")\n",
    "    print(\"Likelihood Scores:\")\n",
    "    print(f\"Zohair's probability: {zohair_term}\")\n",
    "    print(f\"Ateeb's probability: {ateeb_term}\")\n",
    "\n",
    "    # Determine the most likely author\n",
    "    if zohair_term > ateeb_term:\n",
    "        print(\"The document was most likely written by Zohair.\")\n",
    "    elif ateeb_term > zohair_term:\n",
    "        print(\"The document was most likely written by Ateeb.\")\n",
    "    else:\n",
    "        print(\"The document's author is inconclusive.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
